#!/bin/bash
#
# zenOS Offline Setup Script
# Sets up local AI models for true offline operation
#

set -e

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

echo -e "${CYAN}"
cat << "EOF"
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   ğŸ§˜ zenOS Offline Mode Setup ğŸ”Œ  â•‘
    â•‘   True AI - No Internet Required  â•‘
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EOF
echo -e "${NC}"

# Detect platform
detect_platform() {
    if [ -n "$TERMUX_VERSION" ] || [ -d "/data/data/com.termux" ]; then
        echo "termux"
    elif [ "$(uname)" == "Darwin" ]; then
        echo "macos"
    elif [ "$(expr substr $(uname -s) 1 5)" == "Linux" ]; then
        echo "linux"
    else
        echo "unknown"
    fi
}

PLATFORM=$(detect_platform)
echo -e "${BLUE}ğŸ” Detected platform: ${YELLOW}$PLATFORM${NC}"

# Function to install Ollama
install_ollama() {
    echo -e "${YELLOW}ğŸ“¦ Installing Ollama...${NC}"
    
    case $PLATFORM in
        termux)
            # Termux installation (experimental but works!)
            echo -e "${CYAN}Setting up Ollama for Termux...${NC}"
            
            # Install Go if not present
            if ! command -v go &> /dev/null; then
                pkg install -y golang
            fi
            
            # Clone and build Ollama
            if [ ! -d "$HOME/ollama" ]; then
                cd $HOME
                git clone https://github.com/ollama/ollama
                cd ollama
                
                # Build with mobile optimizations
                CGO_ENABLED=1 go build -tags "mobile" .
                
                # Create symlink
                ln -sf $HOME/ollama/ollama $PREFIX/bin/ollama
            fi
            
            # Start Ollama service in background
            nohup $HOME/ollama/ollama serve > /dev/null 2>&1 &
            echo $! > $HOME/.ollama.pid
            
            echo -e "${GREEN}âœ… Ollama installed for Termux!${NC}"
            ;;
            
        linux|macos)
            # Standard installation
            if ! command -v ollama &> /dev/null; then
                echo "Installing Ollama..."
                curl -fsSL https://ollama.com/install.sh | sh
            else
                echo -e "${GREEN}âœ… Ollama already installed${NC}"
            fi
            
            # Start Ollama service
            ollama serve > /dev/null 2>&1 &
            ;;
            
        *)
            echo -e "${RED}âŒ Unsupported platform${NC}"
            exit 1
            ;;
    esac
    
    # Wait for Ollama to start
    echo -e "${YELLOW}â³ Waiting for Ollama to start...${NC}"
    for i in {1..30}; do
        if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            echo -e "${GREEN}âœ… Ollama is running!${NC}"
            break
        fi
        sleep 1
    done
}

# Function to install llama.cpp as fallback
install_llamacpp() {
    echo -e "${YELLOW}ğŸ“¦ Installing llama.cpp (fallback)...${NC}"
    
    case $PLATFORM in
        termux)
            # Termux build
            pkg install -y clang make
            
            if [ ! -d "$HOME/llama.cpp" ]; then
                cd $HOME
                git clone https://github.com/ggerganov/llama.cpp
                cd llama.cpp
                
                # Build with Android optimizations
                make LLAMA_PORTABLE=1 LLAMA_OPENBLAS=1
                
                # Create symlinks
                ln -sf $HOME/llama.cpp/main $PREFIX/bin/llama
                ln -sf $HOME/llama.cpp/quantize $PREFIX/bin/llama-quantize
            fi
            
            echo -e "${GREEN}âœ… llama.cpp installed!${NC}"
            ;;
            
        linux|macos)
            if [ ! -d "$HOME/llama.cpp" ]; then
                cd $HOME
                git clone https://github.com/ggerganov/llama.cpp
                cd llama.cpp
                make
                
                # Add to PATH
                echo 'export PATH="$HOME/llama.cpp:$PATH"' >> ~/.bashrc
            fi
            ;;
    esac
}

# Function to download mobile-friendly models
download_models() {
    echo -e "${CYAN}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  ğŸ“± Select Models to Download"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo -e "${NC}"
    
    # Get available RAM
    if [ "$PLATFORM" == "termux" ]; then
        AVAILABLE_RAM=$(cat /proc/meminfo | grep MemAvailable | awk '{print int($2/1024)}')
    else
        AVAILABLE_RAM=8000
    fi
    
    echo -e "${BLUE}Available RAM: ${YELLOW}${AVAILABLE_RAM}MB${NC}"
    echo ""
    
    # Model options based on RAM
    if [ $AVAILABLE_RAM -lt 2000 ]; then
        echo -e "${YELLOW}âš ï¸  Limited RAM - Recommending ultra-light models:${NC}"
        MODELS=("qwen:0.5b" "tinyllama")
    elif [ $AVAILABLE_RAM -lt 4000 ]; then
        echo -e "${GREEN}âœ… Mobile models recommended:${NC}"
        MODELS=("tinyllama" "phi-2" "gemma:2b" "stablelm2:1.6b")
    else
        echo -e "${GREEN}âœ… All models available:${NC}"
        MODELS=("tinyllama" "phi-2" "gemma:2b" "mistral:7b" "llama3:8b")
    fi
    
    echo "Available models:"
    for i in "${!MODELS[@]}"; do
        echo "  $((i+1)). ${MODELS[$i]}"
    done
    echo "  0. Skip model download"
    echo ""
    
    read -p "Select models to download (space-separated numbers): " -a selections
    
    for selection in "${selections[@]}"; do
        if [ "$selection" == "0" ]; then
            echo "Skipping model download"
            break
        elif [ "$selection" -ge 1 ] && [ "$selection" -le "${#MODELS[@]}" ]; then
            MODEL="${MODELS[$((selection-1))]}"
            echo -e "${YELLOW}ğŸ“¥ Downloading $MODEL...${NC}"
            
            if command -v ollama &> /dev/null; then
                ollama pull "$MODEL"
                echo -e "${GREEN}âœ… $MODEL ready!${NC}"
            else
                echo -e "${RED}âŒ Ollama not available${NC}"
            fi
        fi
    done
}

# Function to create offline launcher
create_launcher() {
    echo -e "${YELLOW}ğŸš€ Creating offline launcher...${NC}"
    
    cat > $HOME/zen-offline << 'EOF'
#!/bin/bash
# zenOS Offline Mode Launcher

# Force offline mode
export ZEN_PREFER_OFFLINE=true
export OLLAMA_HOST=${OLLAMA_HOST:-http://localhost:11434}

# Start Ollama if not running
if ! curl -s $OLLAMA_HOST/api/tags > /dev/null 2>&1; then
    echo "Starting Ollama..."
    if [ -f $HOME/.ollama.pid ]; then
        kill $(cat $HOME/.ollama.pid) 2>/dev/null || true
    fi
    nohup ollama serve > /dev/null 2>&1 &
    echo $! > $HOME/.ollama.pid
    sleep 3
fi

# List available models
echo "ğŸ¤– Available offline models:"
curl -s $OLLAMA_HOST/api/tags | grep -o '"name":"[^"]*"' | cut -d'"' -f4 | sed 's/^/  - /'

echo ""
echo "ğŸ§˜ Starting zenOS in offline mode..."

# Run zenOS with offline provider
cd $HOME/zenOS
python -m zen.cli chat --offline "$@"
EOF
    
    chmod +x $HOME/zen-offline
    
    # Add to PATH if not already there
    if ! grep -q 'export PATH="$HOME:$PATH"' $HOME/.bashrc; then
        echo 'export PATH="$HOME:$PATH"' >> $HOME/.bashrc
    fi
    
    echo -e "${GREEN}âœ… Offline launcher created!${NC}"
    echo "   Run 'zen-offline' to start in offline mode"
}

# Function to test offline setup
test_offline() {
    echo -e "${YELLOW}ğŸ§ª Testing offline setup...${NC}"
    
    # Check if Ollama is running
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo -e "${GREEN}âœ… Ollama server is running${NC}"
        
        # List models
        MODELS=$(curl -s http://localhost:11434/api/tags | grep -o '"name":"[^"]*"' | cut -d'"' -f4)
        
        if [ -z "$MODELS" ]; then
            echo -e "${YELLOW}âš ï¸  No models installed yet${NC}"
        else
            echo -e "${GREEN}âœ… Installed models:${NC}"
            echo "$MODELS" | sed 's/^/  - /'
            
            # Test generation
            FIRST_MODEL=$(echo "$MODELS" | head -n1)
            echo -e "${YELLOW}Testing generation with $FIRST_MODEL...${NC}"
            
            RESPONSE=$(curl -s http://localhost:11434/api/generate \
                -d "{\"model\":\"$FIRST_MODEL\",\"prompt\":\"Say hello\",\"stream\":false}" \
                | grep -o '"response":"[^"]*"' | cut -d'"' -f4)
            
            if [ -n "$RESPONSE" ]; then
                echo -e "${GREEN}âœ… Response: $RESPONSE${NC}"
            else
                echo -e "${RED}âŒ Generation failed${NC}"
            fi
        fi
    else
        echo -e "${RED}âŒ Ollama is not running${NC}"
    fi
}

# Main installation flow
main() {
    echo -e "${CYAN}Starting offline setup...${NC}"
    echo ""
    
    # Step 1: Install Ollama
    if [ "$1" != "--skip-install" ]; then
        install_ollama
        
        # Fallback: Install llama.cpp if Ollama fails
        if ! command -v ollama &> /dev/null; then
            echo -e "${YELLOW}Ollama not available, installing llama.cpp as fallback...${NC}"
            install_llamacpp
        fi
    fi
    
    # Step 2: Download models
    if [ "$1" != "--skip-models" ]; then
        download_models
    fi
    
    # Step 3: Create launcher
    create_launcher
    
    # Step 4: Test setup
    test_offline
    
    echo ""
    echo -e "${GREEN}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "    ğŸ‰ Offline Setup Complete! ğŸ‰"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo -e "${NC}"
    echo -e "${CYAN}Quick Start:${NC}"
    echo "  â€¢ Run ${YELLOW}zen-offline${NC} to start in offline mode"
    echo "  â€¢ Run ${YELLOW}ollama list${NC} to see installed models"
    echo "  â€¢ Run ${YELLOW}ollama pull <model>${NC} to add more models"
    echo ""
    echo -e "${BLUE}Recommended mobile models:${NC}"
    echo "  â€¢ ${GREEN}tinyllama${NC} - Ultra fast (637MB)"
    echo "  â€¢ ${GREEN}phi-2${NC} - Best quality/size (1.6GB)"
    echo "  â€¢ ${GREEN}qwen:0.5b${NC} - Tiny but capable (395MB)"
    echo ""
    echo -e "${YELLOW}You now have AI that works anywhere, anytime!${NC}"
    echo -e "${GREEN}No internet? No problem! ğŸ§˜${NC}"
}

# Handle arguments
case "$1" in
    --test)
        test_offline
        ;;
    --models)
        download_models
        ;;
    --help)
        echo "Usage: $0 [OPTIONS]"
        echo "Options:"
        echo "  --test         Test offline setup"
        echo "  --models       Download models only"
        echo "  --skip-install Skip installation"
        echo "  --skip-models  Skip model download"
        echo "  --help         Show this help"
        ;;
    *)
        main "$@"
        ;;
esac
